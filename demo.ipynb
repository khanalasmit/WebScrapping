{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen,urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.error import URLError,HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractdata(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        for child in bs.find('table',{'id':'giftList'}).tr.next_siblings:\n",
    "            print(child)\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred: {e}\")\n",
    "    except URLError as e:\n",
    "        print(f\"URLError occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "extractdata(\"https://www.pythonscraping.com/pages/page3.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extractdata1(url):\n",
    "    try:\n",
    "        html=urlopen(url)\n",
    "        bs=BeautifulSoup(html,'html.parser')\n",
    "        print(bs.find('img', {'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())\n",
    "    except URLError:\n",
    "        print(\"couldnot acess the url\")\n",
    "    except HTTPError as e:\n",
    "        print(f\"the error is:{e}\")\n",
    "\n",
    "extractdata1(\"https://www.pythonscraping.com/pages/page3.html\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.pythonscraping.com/pages/page3.html\"\n",
    "html=urlopen(url)\n",
    "bs=BeautifulSoup(html,'html.parser')\n",
    "images=bs.find_all('img',{'src':re.compile(r\"^.+\\/img\\/.+\\/img.*\\.jpg$\")})\n",
    "for image in images:\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part=bs.find('div',{'id':'footer'})\n",
    "print(first_part.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url='http://en.wikipedia.org/wiki/Kevin_Bacon'\n",
    "html=urlopen(url)\n",
    "bs=BeautifulSoup(html,'html.parser')\n",
    "for link in bs.find('div',{'id':'bodyContent'}).find_all('a',href=re.compile(r\"^(/wiki/)((?!:).)*$\")):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "random.seed(datetime.datetime.now().timestamp())\n",
    "def getLinks(articleUrl):\n",
    "    url=f\"http://en.wikipedia.org{articleUrl}\"\n",
    "    html=urlopen(url)\n",
    "    bs=BeautifulSoup(html,'html.parser')\n",
    "    return bs.find('div',{'id':'bodyContent'}).find_all('a',href=re.compile(r\"^(/wiki/)((?!:).)*$\"))\n",
    "\n",
    "links=getLinks('/wiki/Kevin_Bacon')\n",
    "count=0\n",
    "while count<=30:\n",
    "    newArticle=links[random.randint(0,len(links)-1)].attrs['href']\n",
    "    links=getLinks(newArticle)\n",
    "    count+=1\n",
    "print(newArticle)\n",
    "url=f\"http://en.wikipedia.org/wiki/Bruno_Urli%C4%87\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages=set()\n",
    "def getLinks(pageUrl):\n",
    "    html=urlopen(f'https://en.wikipedia.org{pageUrl}')\n",
    "    bs=BeautifulSoup(html,'html.parser')\n",
    "    try:\n",
    "        print(bs.h1.get_text())\n",
    "\n",
    "    except AttributeError:\n",
    "        print(\"this page is missing something!!Continuing\")\n",
    "    \n",
    "    for link in bs.find_all('a',href=re.compile(\"^(/wiki/)\")):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                newPage=link.attrs['href']\n",
    "                print('-'*20)\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)                \n",
    "articleUrl='/wiki/Emma_Watson'\n",
    "getLinks(articleUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInternalLinks(bs,url):\n",
    "    netloc=urlparse(url).netloc\n",
    "    scheme=urlparse(url).scheme\n",
    "    internalLinks=set()\n",
    "    for link in bs.find_all('a'):\n",
    "        if not links.attrs.get('href'):\n",
    "            continue\n",
    "        parsed=urlparse(link.attrs['href'])\n",
    "        if parsed.netloc=='':\n",
    "            l=f'{scheme}://{netloc}/{link.attrs[\"href\"].strip(\"/\")}'\n",
    "            internalLinks.add(l)\n",
    "        elif parsed.netloc == internal_netloc:\n",
    "             internalLinks.add(link.attrs['href']) \n",
    "    return list(internalLinks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInternalLinks(bs, url): \n",
    "    netloc = urlparse(url).netloc \n",
    "    scheme = urlparse(url).scheme \n",
    "    internalLinks = set() \n",
    "    for link in bs.find_all('a'): \n",
    "        if not link.attrs.get('href'): \n",
    "            continue \n",
    "        parsed = urlparse(link.attrs['href']) \n",
    "        if parsed.netloc == '': \n",
    "            l = f'{scheme}://{netloc}/{link.attrs[\"href\"].strip(\"/\")}' \n",
    "            internalLinks.add(l) \n",
    "        elif parsed.netloc == netloc: \n",
    "            internalLinks.add(link.attrs['href']) \n",
    "    return list(internalLinks) \n",
    "def getExternalLinks(bs, url): \n",
    "    internal_netloc = urlparse(url).netloc \n",
    "    externalLinks = set() \n",
    "    for link in bs.find_all('a'): \n",
    "        if not link.attrs.get('href'): \n",
    "            continue \n",
    "        parsed = urlparse(link.attrs['href']) \n",
    "        if parsed.netloc != '' and parsed.netloc != internal_netloc: \n",
    "            externalLinks.add(link.attrs['href']) \n",
    "    return list(externalLinks) \n",
    "\n",
    "def getRandomExternalLink(startingPage): \n",
    "    bs = BeautifulSoup(urlopen(startingPage), 'html.parser') \n",
    "    externalLinks = getExternalLinks(bs, startingPage) \n",
    "    if not len(externalLinks): \n",
    "        print('No external links, looking around the site for one') \n",
    "        internalLinks = getInternalLinks(bs, startingPage) \n",
    "        return getRandomExternalLink(random.choice(internalLinks)) \n",
    "    else: \n",
    "        return random.choice(externalLinks)\n",
    "    \n",
    "def followExternalOnly(startingSite): \n",
    "    externalLink = getRandomExternalLink(startingSite) \n",
    "    print(f'Random external link is: {externalLink}') \n",
    "    followExternalOnly(externalLink) \n",
    "\n",
    "followExternalOnly('https://www.oreilly.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    def __init__(self,url,title,body):\n",
    "        self.url=url\n",
    "        self.title=title.strip()\n",
    "        self.body=body.strip()\n",
    "    \n",
    "    def print(self):\n",
    "        print(\"Url:\",self.url)\n",
    "        print(\"Title:\",self.title)\n",
    "        print(\"Body:\",self.body)\n",
    "        \n",
    "class Website:\n",
    "    def __init__(self,name,url,titletag,bodytag):\n",
    "        self.name=name\n",
    "        self.url=url\n",
    "        self.titletag=titletag\n",
    "        self.bodytag=bodytag\n",
    "\n",
    "class Crawler:\n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception:\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    def safeGet(bs, selector):\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "    \n",
    "    def getContent(website, path):\n",
    "        url = website.url + path\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, website.titletag)\n",
    "            body = Crawler.safeGet(bs, website.bodytag)\n",
    "            return Content(url, title, body)\n",
    "        return Content(url, '', '')   \n",
    "\n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapCNN(url):\n",
    "    bs=BeautifulSoup(urlopen(url))\n",
    "    title=bs.find('h1').text\n",
    "    try:\n",
    "        body=bs.find('div',{'class':'article__content'}).text\n",
    "    except HTTPError:\n",
    "        print(\"something of the error occured that is the http error\")\n",
    "    except URLError:\n",
    "        print(\"URl error occured\")\n",
    "    except AttributeError:\n",
    "        print(\"AttributeError occured\")\n",
    "    return Content(url,title,body)\n",
    "\n",
    "content=scrapCNN(\"https://edition.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\")  \n",
    "content.print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self,url,name,targetpattern,absoluteurl,titleTag):\n",
    "        self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
